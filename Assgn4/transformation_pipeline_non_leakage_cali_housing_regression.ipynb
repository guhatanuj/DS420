{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aabca7a",
   "metadata": {},
   "source": [
    "This note is extended from the midterm practice notebook with a focus on <b><i>transformation pipeline</i> </b>\n",
    "\n",
    "It is still to train a linear regressor. We will use Ridge regression with Stochastic Gradient Descent (class SGDRegressor)\n",
    "\n",
    "The dataset is the housing mentioned in chapter 2.\n",
    "\n",
    "Here are steps we need to do\n",
    "\n",
    "1. download the raw dataset\n",
    "\n",
    "2. create train, test sets. We will use cross-validation for the purpose of the dev set.\n",
    "\n",
    "3. prepare data for training: <b>all the steps in this stage are organized in a pipeline</b>\n",
    "\n",
    "    + handle missing values, using median for numerical features, the most frequent category for categorical features\n",
    "        \n",
    "    + transform text -> one-hot vectors\n",
    "    \n",
    "    + scale all features, using StandardScaler\n",
    "\n",
    "\n",
    "4. train a model:\n",
    "    \n",
    "    + model choice: SGDRegressor\n",
    "    \n",
    "    + using grid search for hyperparameter tuning: learning rate and regularization (L2 norm) coefficent.\n",
    "    \n",
    "5. evaluate the final model, using RMSE metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74c6eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helping functions\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "from sklearn import set_config\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a545675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    \n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "def train_test_split(X, y, test_ratio = 0.2):\n",
    "    total_size = len(X)\n",
    "    print(total_size)\n",
    "\n",
    "    test_size = int(total_size * test_ratio)\n",
    "    \n",
    "    \n",
    "    train_size = total_size - test_size\n",
    "\n",
    "    np.random.seed(42)\n",
    "    rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "    X_train = X.iloc[rnd_indices[:train_size]]\n",
    "    y_train = y.iloc[rnd_indices[:train_size]]\n",
    "    \n",
    "    X_test = X.iloc[rnd_indices[train_size:]]\n",
    "    y_test = y.iloc[rnd_indices[train_size:]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c9422a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n",
      "(15799, 10)\n",
      "Training ...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "Training done, best hyperparams: \n",
      "{'alpha': 0.001, 'eta0': 0.001, 'max_iter': 1000, 'penalty': 'l2', 'random_state': 42}\n",
      "test RMSE:  68927.76074212731\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    #1 load data\n",
    "    \n",
    "    housing = load_housing_data() # housing is a dataframe\n",
    "    housing_X = housing.drop(\"median_house_value\", axis=1)\n",
    "    housing_y = housing[\"median_house_value\"].copy()\n",
    "    \n",
    "    #5--- Attempting Bonus Here \n",
    "    housing = housing.assign(bedrooms_ratio = housing.total_bedrooms/housing.total_rooms)\n",
    "    housing = housing.assign(rooms_per_house = housing.total_rooms/housing.households)\n",
    "    housing = housing.assign(people_per_house = housing.population / housing.households)\n",
    "    \n",
    "    #2 split train, test sets\n",
    "    \n",
    "    housing_Xtrain, housing_Xtest, housing_ytrain, housing_ytest = train_test_split(housing_X, housing_y, test_ratio = 0.2)\n",
    "    \n",
    "    #---Outlier Removal \n",
    "    df = pd.concat([housing_Xtrain,housing_ytrain], axis=1) #join training set back together\n",
    "    newdf = df.select_dtypes(include=np.number) #drop non-numerical column\n",
    "    newdf = newdf.drop(\"total_bedrooms\", axis='columns') #dropped bedrooms because for some reason Zscaler was throwing errors \n",
    "    \n",
    "    newdf = newdf[(np.abs(stats.zscore(newdf)) < 3).all(axis=1)]\n",
    "    #the above first finds z values of all data points, then drops the ones more than 3 std devs\n",
    "    \n",
    "    df = df[df.index.isin(newdf.index)]\n",
    "    housing_X_train = df.drop(\"median_house_value\", axis=1)\n",
    "    housing_y_train = df[\"median_house_value\"].copy() \n",
    "    \n",
    "    #3 prepare data for training\n",
    "    \n",
    "    #---using transformation pipeline instead of the manual way we did in the midterm practice\n",
    "    #---handle missing values\n",
    "    #---StandardScaler for numerical values\n",
    "    #---transform categorical feature -> one-hot vector\n",
    "    \n",
    "    num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\",\\\n",
    "                  \"population\", \"households\", \"median_income\"]\n",
    "    \n",
    "    cat_attribs = [\"ocean_proximity\"]\n",
    "    \n",
    "    num_pipeline = make_pipeline(SimpleImputer(strategy = 'median'),\\\n",
    "                                 StandardScaler())\n",
    "    \n",
    "    cat_pipeline = make_pipeline(SimpleImputer(strategy = \"most_frequent\"),\\\n",
    "                                 OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "    preprocessing = ColumnTransformer([(\"num\", num_pipeline, num_attribs),\\\n",
    "                                       (\"cat\", cat_pipeline, cat_attribs)], )\n",
    "    \n",
    "    \n",
    "    #4 train\n",
    "    \n",
    "    m = len(housing_Xtrain)\n",
    "    \n",
    "    param_grid = {'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1],  \n",
    "                  'eta0': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "                  'penalty':['l2'],\n",
    "                  'random_state': [42],\n",
    "                  'max_iter':[1000]}  \n",
    "\n",
    "    grid = GridSearchCV(SGDRegressor(), param_grid, refit = True, verbose = 1,n_jobs=-1, cv = 3) \n",
    "    \n",
    "    full_pipeline = Pipeline([(\"preprocessing\", preprocessing),(\"grid_search\", grid)])\n",
    "\n",
    "\n",
    "    print(\"Training ...\")\n",
    "\n",
    "    full_pipeline.fit(housing_Xtrain, housing_ytrain)\n",
    "    \n",
    "    # print best parameter after tuning \n",
    "    \n",
    "    print(\"Training done, best hyperparams: \")\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    # evaluate the final model\n",
    "    \n",
    "    final_predictions = full_pipeline.predict(housing_Xtest)\n",
    "\n",
    "    final_rmse = mean_squared_error(housing_ytest, final_predictions, squared=False)\n",
    "    print(\"test RMSE: \", final_rmse) # prints\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e75907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
