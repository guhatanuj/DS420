{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aabca7a",
   "metadata": {},
   "source": [
    "<b>This note is to train a linear regressor. We will use Ridge regression with Stochastic Gradient Descent (class SGDRegressor)</b>\n",
    "\n",
    "The dataset is the housing mentioned in chapter 2.\n",
    "\n",
    "Here are steps we need to do\n",
    "\n",
    "1. download the raw dataset\n",
    "\n",
    "2. create train, test sets. We will use cross-validation for the purpose of the dev set.\n",
    "\n",
    "3. prepare data for training\n",
    "\n",
    "    + handle missing values, using median for numerical features, the most frequent category for categorical features\n",
    "    \n",
    "    + transform text -> one-hot vectors\n",
    "    \n",
    "    + scale all features, using StandardScaler\n",
    "\n",
    "\n",
    "4. train a model:\n",
    "    \n",
    "    + model choice: SGDRegressor\n",
    "    \n",
    "    + using grid search for hyperparameter tuning: learning rate and regularization (L2 norm) coefficent.\n",
    "    \n",
    "5. evaluate the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c6eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helping functions\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a545675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data():\n",
    "   \n",
    "    pd.read_csv(\"housing.csv\")\n",
    "    #---handle text column\n",
    "    return pd.get_dummies(housing)\n",
    "\n",
    "def train_test_split(X, y, test_ratio = 0.2):\n",
    "    total_size = len(X)\n",
    "    print(total_size)\n",
    "\n",
    "    test_size = int(total_size * test_ratio)\n",
    "    train_size = total_size - test_size\n",
    "\n",
    "    np.random.seed(42)\n",
    "    rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "    X_train = X.iloc[rnd_indices[:train_size]]\n",
    "    y_train = y.iloc[rnd_indices[:train_size]]\n",
    "    X_test = X.iloc[rnd_indices[train_size:]]\n",
    "    y_test = y.iloc[rnd_indices[train_size:]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def fill_na(X, strategy = 'median'):\n",
    "    #X: ndarray array of shape (n_samples, n_features)\n",
    "    #return ndarray of shape (n_samples, n_features) with missing values filled by the strategy\n",
    "\n",
    "    imputer = SimpleImputer(strategy = strategy)\n",
    "    imputer.fit(X)\n",
    "\n",
    "    return imputer.transform(X)\n",
    "\n",
    "def get_outlier_indices(X):\n",
    "    #X: ndarray of shape (n_samples, n_features)\n",
    "    #y: label of shape (n_samples, k = 1)\n",
    "    #return the X and y with ourliers dropped\n",
    "    \n",
    "    isolation_forest = IsolationForest(random_state = 42)\n",
    "    outlier_pred = isolation_forest.fit_predict(X)\n",
    "\n",
    "    return outlier_pred\n",
    "\n",
    "def standard_scaler(X):\n",
    "    #scaling all columns in X such that for each column, we have mean = 0, std = 1\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    return std_scaler.fit_transform(X)\n",
    "\n",
    "def ordinal_encoder(df_one_column):\n",
    "    #df_one_column: a dataframe with one categorical column\n",
    "    #return an array of numbers representing categories \n",
    "    \n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    return ordinal_encoder.fit_transform(df_one_column)\n",
    "\n",
    "def one_hot_encoder(df_one_column):\n",
    "    #df_one_column: a dataframe with one categorical column\n",
    "    #return a 2d array of 0 / 1\n",
    "    \n",
    "    cat_encoder = OneHotEncoder(sparse = False)\n",
    "    return cat_encoder.fit_transform(df_one_column)\n",
    "\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    \n",
    "    # defining parameter range \n",
    "    m = len(X_train)\n",
    "    \n",
    "    param_grid = {'alpha': [0.1/m, 1/m, 10/m, 100/m],  \n",
    "                  'eta0': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "                  'penalty':['l2'],\n",
    "                  'random_state': [42],\n",
    "                  'max_iter':[1000]}  \n",
    "\n",
    "    print(\"Training ...\")\n",
    "    grid = GridSearchCV(SGDRegressor(), param_grid, refit = True, verbose = 1,n_jobs=-1, cv = 3) \n",
    "\n",
    "    # fitting the model for grid search \n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    # print best parameter after tuning \n",
    "    print(\"Training done\")\n",
    "    print(grid.best_params_)\n",
    "    return grid\n",
    "\n",
    "#    grid_predictions = grid.predict(X_test) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #sgd_reg = SGDRegressor() #(penalty=\"l2\", alpha=0.1 / m, tol=None, max_iter=1000, eta0=0.01, random_state=42)\n",
    "\n",
    "    #sgd_reg.fit(X, y.ravel()) # y.ravel() because fit() expects 1D targets\n",
    "    \n",
    "class DisplayLossCurve(object):\n",
    "  def __init__(self, print_loss=False):\n",
    "    self.print_loss = print_loss\n",
    "\n",
    "  \"\"\"Make sure the model verbose is set to 1\"\"\"\n",
    "  def __enter__(self):\n",
    "    self.old_stdout = sys.stdout\n",
    "    sys.stdout = self.mystdout = io.StringIO()\n",
    "  \n",
    "  def __exit__(self, *args, **kwargs):\n",
    "    sys.stdout = self.old_stdout\n",
    "    loss_history = self.mystdout.getvalue()\n",
    "    loss_list = []\n",
    "    for line in loss_history.split('\\n'):\n",
    "      if(len(line.split(\"loss: \")) == 1):\n",
    "        continue\n",
    "      loss_list.append(float(line.split(\"loss: \")[-1]))\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(loss_list)), loss_list)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    if self.print_loss:\n",
    "      print(\"=============== Loss Array ===============\")\n",
    "      print(np.array(loss_list))\n",
    "      \n",
    "    return True\n",
    "    \n",
    "def main():\n",
    "\n",
    "    #1 load data\n",
    "    \n",
    "    housing = load_housing_data() # housing is a dataframe\n",
    "       \n",
    "   \n",
    "\n",
    "           \n",
    "    housing_X = housing.drop(\"median_house_value\", axis=1)\n",
    "    housing_y = housing[\"median_house_value\"].copy()\n",
    "\n",
    "    \n",
    "    #2 split train, test sets\n",
    "    \n",
    "    housing_Xtrain, housing_Xtest, housing_ytrain, housing_ytest = train_test_split(housing_X, housing_y, test_ratio = 0.2)\n",
    "    \n",
    "    #3 prepare data for training\n",
    "    \n",
    "    #---handle missing values\n",
    "     \n",
    "    housing_Xtrain_num = housing_Xtrain.select_dtypes(include=[np.number])\n",
    "    housing_Xtest_num = housing_Xtest.select_dtypes(include=[np.number])\n",
    "    \n",
    "    housing_Xtrain_num_columns = housing_Xtrain_num.columns\n",
    "    \n",
    "    housing_Xtrain_num = fill_na(housing_Xtrain_num)\n",
    "    housing_Xtest_num = fill_na(housing_Xtest_num)\n",
    "    \n",
    "    #---drop outlier in training sets\n",
    "\n",
    "    outlier_indices = get_outlier_indices(housing_Xtrain_num)\n",
    "    housing_Xtrain = housing_Xtrain.iloc[outlier_indices == 1]\n",
    "    housing_ytrain = housing_ytrain.iloc[outlier_indices == 1]\n",
    "\n",
    "\n",
    "    housing_Xtrain_num = pd.DataFrame(housing_Xtrain_num, columns=housing_Xtrain_num_columns)\n",
    "    housing_Xtest_num = pd.DataFrame(housing_Xtest_num, columns=housing_Xtrain_num_columns)\n",
    "\n",
    "    housing_Xtrain_num = housing_Xtrain_num.iloc[outlier_indices == 1]\n",
    "\n",
    "    \n",
    "    \n",
    "    #---Scale\n",
    "    \n",
    "    housing_Xtrain_num = standard_scaler(housing_Xtrain_num)\n",
    "    housing_Xtest_num = standard_scaler(housing_Xtest_num)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #4 train\n",
    "    \n",
    "    final_model = train(housing_Xtrain_num, housing_ytrain.values)\n",
    "    \n",
    "    #5 evaluate the final model\n",
    "    \n",
    "    final_predictions = final_model.predict(housing_Xtest_num)\n",
    "    final_rmse = mean_squared_error(housing_ytest, final_predictions, squared=False)\n",
    "    print(\"RMSE: \", final_rmse) # prints 41424.40026462184\n",
    "    \n",
    "    #plot\n",
    "    best_model = SGDRegressor(**final_model.best_params_, verbose = 1)\n",
    "    with DisplayLossCurve(print_loss=True):\n",
    "        best_model.fit(housing_Xtrain_num, housing_ytrain.values)\n",
    "        #I was only able to plot loss curve for training dataset, could not figure out loss curve for dev set since we were using gridsearch.\n",
    "    \n",
    "    #accuracy part of Q6, Q6 also addressed in text box below\n",
    "    final_R2 = r2_score(housing_ytest, final_predictions)\n",
    "    print(\"R2: \", final_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9422a69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m#1 load data\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     housing \u001b[38;5;241m=\u001b[39m \u001b[43mload_housing_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# housing is a dataframe\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     housing_X \u001b[38;5;241m=\u001b[39m housing\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian_house_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    132\u001b[0m     housing_y \u001b[38;5;241m=\u001b[39m housing[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian_house_value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mload_housing_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhousing.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#---handle text column\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mget_dummies(\u001b[43mhousing\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'housing' is not defined"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d26826",
   "metadata": {},
   "source": [
    "6. The goodness of fit score indicates that the model can definitely be improved, before the bonus feature engineering the R2 is 0.61. Since Linear classification systems are often sensitive to the number and nature of regressor variables, I added the three ratio feautres and now the R2 score is 0.64, marginally better. Maybe we should try to apply a different kind of model to this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608d52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
