{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aabca7a",
   "metadata": {},
   "source": [
    "<b>This note is to train a linear regressor. We will use Ridge regression with Stochastic Gradient Descent (class SGDRegressor)</b>\n",
    "\n",
    "The dataset is the housing mentioned in chapter 2.\n",
    "\n",
    "Here are steps we need to do\n",
    "\n",
    "1. download the raw dataset\n",
    "\n",
    "2. create train, test sets. We will use cross-validation for the purpose of the dev set.\n",
    "\n",
    "3. prepare data for training\n",
    "\n",
    "    + handle missing values, using median for numerical features, the most frequent category for categorical features\n",
    "    \n",
    "    + transform text -> one-hot vectors\n",
    "    \n",
    "    + scale all features, using StandardScaler\n",
    "\n",
    "\n",
    "4. train a model:\n",
    "    \n",
    "    + model choice: SGDRegressor\n",
    "    \n",
    "    + using grid search for hyperparameter tuning: learning rate and regularization (L2 norm) coefficent.\n",
    "    \n",
    "5. evaluate the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74c6eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helping functions\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a545675a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity_&lt;1H OCEAN</th>\n",
       "      <th>ocean_proximity_INLAND</th>\n",
       "      <th>ocean_proximity_ISLAND</th>\n",
       "      <th>ocean_proximity_NEAR BAY</th>\n",
       "      <th>ocean_proximity_NEAR OCEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>-121.09</td>\n",
       "      <td>39.48</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>1.5603</td>\n",
       "      <td>78100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>-121.21</td>\n",
       "      <td>39.49</td>\n",
       "      <td>18.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2.5568</td>\n",
       "      <td>77100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>-121.22</td>\n",
       "      <td>39.43</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>92300.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>-121.32</td>\n",
       "      <td>39.43</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>1.8672</td>\n",
       "      <td>84700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>-121.24</td>\n",
       "      <td>39.37</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2785.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>2.3886</td>\n",
       "      <td>89400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0        -122.23     37.88                41.0        880.0           129.0   \n",
       "1        -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2        -122.24     37.85                52.0       1467.0           190.0   \n",
       "3        -122.25     37.85                52.0       1274.0           235.0   \n",
       "4        -122.25     37.85                52.0       1627.0           280.0   \n",
       "...          ...       ...                 ...          ...             ...   \n",
       "20635    -121.09     39.48                25.0       1665.0           374.0   \n",
       "20636    -121.21     39.49                18.0        697.0           150.0   \n",
       "20637    -121.22     39.43                17.0       2254.0           485.0   \n",
       "20638    -121.32     39.43                18.0       1860.0           409.0   \n",
       "20639    -121.24     39.37                16.0       2785.0           616.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "0           322.0       126.0         8.3252            452600.0   \n",
       "1          2401.0      1138.0         8.3014            358500.0   \n",
       "2           496.0       177.0         7.2574            352100.0   \n",
       "3           558.0       219.0         5.6431            341300.0   \n",
       "4           565.0       259.0         3.8462            342200.0   \n",
       "...           ...         ...            ...                 ...   \n",
       "20635       845.0       330.0         1.5603             78100.0   \n",
       "20636       356.0       114.0         2.5568             77100.0   \n",
       "20637      1007.0       433.0         1.7000             92300.0   \n",
       "20638       741.0       349.0         1.8672             84700.0   \n",
       "20639      1387.0       530.0         2.3886             89400.0   \n",
       "\n",
       "       ocean_proximity_<1H OCEAN  ocean_proximity_INLAND  \\\n",
       "0                              0                       0   \n",
       "1                              0                       0   \n",
       "2                              0                       0   \n",
       "3                              0                       0   \n",
       "4                              0                       0   \n",
       "...                          ...                     ...   \n",
       "20635                          0                       1   \n",
       "20636                          0                       1   \n",
       "20637                          0                       1   \n",
       "20638                          0                       1   \n",
       "20639                          0                       1   \n",
       "\n",
       "       ocean_proximity_ISLAND  ocean_proximity_NEAR BAY  \\\n",
       "0                           0                         1   \n",
       "1                           0                         1   \n",
       "2                           0                         1   \n",
       "3                           0                         1   \n",
       "4                           0                         1   \n",
       "...                       ...                       ...   \n",
       "20635                       0                         0   \n",
       "20636                       0                         0   \n",
       "20637                       0                         0   \n",
       "20638                       0                         0   \n",
       "20639                       0                         0   \n",
       "\n",
       "       ocean_proximity_NEAR OCEAN  \n",
       "0                               0  \n",
       "1                               0  \n",
       "2                               0  \n",
       "3                               0  \n",
       "4                               0  \n",
       "...                           ...  \n",
       "20635                           0  \n",
       "20636                           0  \n",
       "20637                           0  \n",
       "20638                           0  \n",
       "20639                           0  \n",
       "\n",
       "[20640 rows x 14 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_housing_data():\n",
    "   \n",
    "    pd.read_csv(\"housing.csv\")\n",
    "    #---handle text column\n",
    "    return pd.get_dummies(housing)\n",
    "\n",
    "def train_test_split(X, y, test_ratio = 0.2):\n",
    "    total_size = len(X)\n",
    "    print(total_size)\n",
    "\n",
    "    test_size = int(total_size * test_ratio)\n",
    "    train_size = total_size - test_size\n",
    "\n",
    "    np.random.seed(42)\n",
    "    rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "    X_train = X.iloc[rnd_indices[:train_size]]\n",
    "    y_train = y.iloc[rnd_indices[:train_size]]\n",
    "    X_test = X.iloc[rnd_indices[train_size:]]\n",
    "    y_test = y.iloc[rnd_indices[train_size:]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def fill_na(X, strategy = 'median'):\n",
    "    #X: ndarray array of shape (n_samples, n_features)\n",
    "    #return ndarray of shape (n_samples, n_features) with missing values filled by the strategy\n",
    "\n",
    "    imputer = SimpleImputer(strategy = strategy)\n",
    "    imputer.fit(X)\n",
    "\n",
    "    return imputer.transform(X)\n",
    "\n",
    "def get_outlier_indices(X):\n",
    "    #X: ndarray of shape (n_samples, n_features)\n",
    "    #y: label of shape (n_samples, k = 1)\n",
    "    #return the X and y with ourliers dropped\n",
    "    \n",
    "    isolation_forest = IsolationForest(random_state = 42)\n",
    "    outlier_pred = isolation_forest.fit_predict(X)\n",
    "\n",
    "    return outlier_pred\n",
    "\n",
    "def standard_scaler(X):\n",
    "    #scaling all columns in X such that for each column, we have mean = 0, std = 1\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    return std_scaler.fit_transform(X)\n",
    "\n",
    "def ordinal_encoder(df_one_column):\n",
    "    #df_one_column: a dataframe with one categorical column\n",
    "    #return an array of numbers representing categories \n",
    "    \n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    return ordinal_encoder.fit_transform(df_one_column)\n",
    "\n",
    "def one_hot_encoder(df_one_column):\n",
    "    #df_one_column: a dataframe with one categorical column\n",
    "    #return a 2d array of 0 / 1\n",
    "    \n",
    "    cat_encoder = OneHotEncoder(sparse = False)\n",
    "    return cat_encoder.fit_transform(df_one_column)\n",
    "\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    \n",
    "    # defining parameter range \n",
    "    m = len(X_train)\n",
    "    \n",
    "    param_grid = {'alpha': [0.1/m, 1/m, 10/m, 100/m],  \n",
    "                  'eta0': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "                  'penalty':['l2'],\n",
    "                  'random_state': [42],\n",
    "                  'max_iter':[1000]}  \n",
    "\n",
    "    print(\"Training ...\")\n",
    "    grid = GridSearchCV(SGDRegressor(), param_grid, refit = True, verbose = 1,n_jobs=-1, cv = 3) \n",
    "\n",
    "    # fitting the model for grid search \n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    # print best parameter after tuning \n",
    "    print(\"Training done\")\n",
    "    print(grid.best_params_)\n",
    "    return grid\n",
    "\n",
    "#    grid_predictions = grid.predict(X_test) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #sgd_reg = SGDRegressor() #(penalty=\"l2\", alpha=0.1 / m, tol=None, max_iter=1000, eta0=0.01, random_state=42)\n",
    "\n",
    "    #sgd_reg.fit(X, y.ravel()) # y.ravel() because fit() expects 1D targets\n",
    "    \n",
    "class DisplayLossCurve(object):\n",
    "  def __init__(self, print_loss=False):\n",
    "    self.print_loss = print_loss\n",
    "\n",
    "  \"\"\"Make sure the model verbose is set to 1\"\"\"\n",
    "  def __enter__(self):\n",
    "    self.old_stdout = sys.stdout\n",
    "    sys.stdout = self.mystdout = io.StringIO()\n",
    "  \n",
    "  def __exit__(self, *args, **kwargs):\n",
    "    sys.stdout = self.old_stdout\n",
    "    loss_history = self.mystdout.getvalue()\n",
    "    loss_list = []\n",
    "    for line in loss_history.split('\\n'):\n",
    "      if(len(line.split(\"loss: \")) == 1):\n",
    "        continue\n",
    "      loss_list.append(float(line.split(\"loss: \")[-1]))\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(loss_list)), loss_list)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    if self.print_loss:\n",
    "      print(\"=============== Loss Array ===============\")\n",
    "      print(np.array(loss_list))\n",
    "      \n",
    "    return True\n",
    "    \n",
    "def main():\n",
    "\n",
    "    #1 load data\n",
    "    \n",
    "    housing = load_housing_data() # housing is a dataframe\n",
    "       \n",
    "    #Attempting the bonus here: \n",
    "    housing = housing.assign(bedrooms_ratio = housing.total_bedrooms/housing.total_rooms)\n",
    "    housing = housing.assign(rooms_per_house = housing.total_rooms/housing.households)\n",
    "    housing = housing.assign(people_per_house = housing.population / housing.households)\n",
    "    \n",
    "           \n",
    "    housing_X = housing.drop(\"median_house_value\", axis=1)\n",
    "    housing_y = housing[\"median_house_value\"].copy()\n",
    "\n",
    "    \n",
    "    #2 split train, test sets\n",
    "    \n",
    "    housing_Xtrain, housing_Xtest, housing_ytrain, housing_ytest = train_test_split(housing_X, housing_y, test_ratio = 0.2)\n",
    "    \n",
    "    #3 prepare data for training\n",
    "    \n",
    "    #---handle missing values\n",
    "     \n",
    "    housing_Xtrain_num = housing_Xtrain.select_dtypes(include=[np.number])\n",
    "    housing_Xtest_num = housing_Xtest.select_dtypes(include=[np.number])\n",
    "    \n",
    "    housing_Xtrain_num_columns = housing_Xtrain_num.columns\n",
    "    \n",
    "    housing_Xtrain_num = fill_na(housing_Xtrain_num)\n",
    "    housing_Xtest_num = fill_na(housing_Xtest_num)\n",
    "    \n",
    "    #---drop outlier in training sets\n",
    "\n",
    "    outlier_indices = get_outlier_indices(housing_Xtrain_num)\n",
    "    housing_Xtrain = housing_Xtrain.iloc[outlier_indices == 1]\n",
    "    housing_ytrain = housing_ytrain.iloc[outlier_indices == 1]\n",
    "\n",
    "\n",
    "    housing_Xtrain_num = pd.DataFrame(housing_Xtrain_num, columns=housing_Xtrain_num_columns)\n",
    "    housing_Xtest_num = pd.DataFrame(housing_Xtest_num, columns=housing_Xtrain_num_columns)\n",
    "\n",
    "    housing_Xtrain_num = housing_Xtrain_num.iloc[outlier_indices == 1]\n",
    "\n",
    "    \n",
    "    \n",
    "    #---Scale\n",
    "    \n",
    "    housing_Xtrain_num = standard_scaler(housing_Xtrain_num)\n",
    "    housing_Xtest_num = standard_scaler(housing_Xtest_num)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #4 train\n",
    "    \n",
    "    final_model = train(housing_Xtrain_num, housing_ytrain.values)\n",
    "    \n",
    "    #5 evaluate the final model\n",
    "    \n",
    "    final_predictions = final_model.predict(housing_Xtest_num)\n",
    "    final_rmse = mean_squared_error(housing_ytest, final_predictions, squared=False)\n",
    "    print(\"RMSE: \", final_rmse) # prints 41424.40026462184\n",
    "    \n",
    "    #plot\n",
    "    best_model = SGDRegressor(**final_model.best_params_, verbose = 1)\n",
    "    with DisplayLossCurve(print_loss=True):\n",
    "        best_model.fit(housing_Xtrain_num, housing_ytrain.values)\n",
    "        #I was only able to plot loss curve for training dataset, could not figure out loss curve for dev set since we were using gridsearch.\n",
    "    \n",
    "    #accuracy part of Q6, Q6 also addressed in text box below\n",
    "    final_R2 = r2_score(housing_ytest, final_predictions)\n",
    "    print(\"R2: \", final_R2)\n",
    "\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c9422a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n",
      "Training ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Training done\n",
      "{'alpha': 0.0006875687568756876, 'eta0': 0.001, 'max_iter': 1000, 'penalty': 'l2', 'random_state': 42}\n",
      "RMSE:  68061.76945252986\n",
      "=============== Loss Array ===============\n",
      "[7.10886503e+09 2.46045678e+09 2.18673029e+09 2.12892016e+09\n",
      " 2.10430113e+09 2.08966302e+09 2.08046236e+09 2.07365290e+09\n",
      " 2.06887666e+09 2.06482542e+09 2.06181909e+09 2.05909803e+09\n",
      " 2.05675513e+09 2.05522001e+09 2.05349982e+09 2.05175419e+09\n",
      " 2.05039960e+09 2.04886547e+09 2.04781669e+09 2.04668273e+09\n",
      " 2.04562547e+09 2.04464014e+09 2.04363513e+09 2.04254531e+09\n",
      " 2.04188146e+09 2.04102386e+09 2.04019608e+09 2.03929703e+09\n",
      " 2.03868572e+09 2.03798781e+09 2.03740114e+09 2.03648140e+09\n",
      " 2.03591576e+09 2.03545868e+09 2.03465410e+09 2.03428227e+09\n",
      " 2.03370941e+09 2.03322717e+09 2.03271396e+09 2.03218846e+09\n",
      " 2.03159878e+09 2.03120469e+09 2.03080333e+09 2.03044746e+09\n",
      " 2.03008356e+09 2.02972016e+09 2.02920843e+09 2.02882321e+09\n",
      " 2.02852992e+09 2.02811679e+09 2.02782933e+09 2.02756638e+09\n",
      " 2.02708382e+09 2.02693728e+09 2.02653341e+09 2.02624001e+09\n",
      " 2.02602018e+09 2.02570844e+09 2.02549078e+09 2.02518056e+09\n",
      " 2.02506507e+09 2.02458673e+09 2.02439018e+09 2.02428201e+09\n",
      " 2.02392288e+09 2.02384443e+09 2.02358867e+09 2.02342079e+09\n",
      " 2.02317319e+09 2.02288037e+09 2.02284310e+09 2.02261317e+09\n",
      " 2.02239174e+09 2.02223239e+09 2.02208574e+09 2.02188747e+09\n",
      " 2.02176293e+09 2.02165198e+09 2.02145140e+09 2.02123780e+09\n",
      " 2.02117622e+09 2.02101668e+09 2.02081805e+09 2.02067077e+09\n",
      " 2.02059482e+09 2.02049543e+09 2.02032814e+09 2.02017763e+09\n",
      " 2.01991719e+09 2.01987291e+09 2.01979468e+09 2.01972184e+09\n",
      " 2.01965580e+09 2.01949173e+09 2.01940110e+09 2.01932666e+09\n",
      " 2.01925647e+09 2.01913002e+09 2.01899224e+09 2.01893493e+09\n",
      " 2.01881298e+09 2.01878102e+09 2.01864972e+09 2.01860442e+09\n",
      " 2.01848543e+09 2.01843911e+09 2.01829965e+09 2.01822907e+09\n",
      " 2.01817380e+09 2.01812303e+09 2.01804868e+09 2.01794977e+09\n",
      " 2.01779680e+09 2.01783307e+09 2.01774965e+09 2.01757544e+09\n",
      " 2.01764139e+09 2.01747427e+09 2.01735844e+09 2.01737241e+09\n",
      " 2.01732265e+09 2.01723057e+09 2.01727182e+09 2.01721577e+09\n",
      " 2.01716170e+09 2.01709617e+09 2.01702686e+09 2.01696498e+09\n",
      " 2.01691854e+09 2.01691198e+09 2.01684299e+09 2.01678853e+09\n",
      " 2.01672393e+09 2.01666258e+09 2.01667804e+09 2.01657362e+09\n",
      " 2.01659267e+09 2.01653031e+09 2.01631997e+09 2.01643735e+09\n",
      " 2.01638097e+09 2.01638954e+09 2.01634362e+09 2.01632727e+09]\n",
      "R2:  0.6397259012606464\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d26826",
   "metadata": {},
   "source": [
    "6. The goodness of fit score indicates that the model can definitely be improved, before the bonus feature engineering the R2 is 0.61. Since Linear classification systems are often sensitive to the number and nature of regressor variables, I added the three ratio feautres and now the R2 score is 0.64, marginally better. Maybe we should try to apply a different kind of model to this dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
